---
title: "ltvProject"
author: "Blake Tian"
output: html_document
---


## R Markdown

Our client is an online greeting card company. The company offers monthly subscriptions at a rate of $1 per month for access to their eCard website. The client is interested in understanding the life-time value (ltv) of their customers.
The life-time value of a customer is defined as the total revenue earned by the company over the course of their relationship with the customer.
The enclosed (synthetic) data represent usage statistics for 10,000 customers. Usage is summarized at a daily level and covers a period of 4 years from 2011-01-01 to 2014-12-31.
The following is a description of each field captured in the enclosed data set containing a total of 10,000 customers.

| Data Field | Description                                                                         | 
|------------|-------------------------------------------------------------------------------------|
| `id`       | A unique user identifier                                                            |
| `status`   | Subscription status ‘0’- new, ‘1’- open, ‘2’- cancelation event                     |
| `gender`   | User gender ‘M’- male, ‘F’- female                                                  |
| `date`     | Date of in which user ‘id’ logged into the site                                     |
| `pages`    | Number of pages visted by user ‘id’ on date ‘date’                                  |
| `onsite`   | Number of minutes spent on site by user ‘id’ on date ‘date’                         |
| `entered`  | Flag indicating whether or not user entered the send order path on date ‘date’      |
| `completed`| Flag indicating whether the user completed the order (sent an eCard)                |
| `holiday`  | Flag indicating whether at least one completed order included a holiday themed card |

We must preprocess the data to determine the following: 

| Data Field | Description                                                                         | 
|------------|-------------------------------------------------------------------------------------|
| `lifespan` | The lifespan of each customer in days, for cancelled customers= cancelDate-openDate,|
|            | for open customers=maxDate-openDate+1/(cancelled customers/total customers)         |


```{r, message=FALSE}
library(tidyverse)
library(ggplot2)
library(ISLR)
library(partykit)
library(caret)
library(rpart)
library(randomForest)
library(pROC)
library(readxl)
library(binaryLogic)
library(dplyr)
library(class)
library(DMwR)
library(nnet)
library(e1071)
library(ranger)
library(glmnet)
library(cluster)
library(ggdendro)

```

```{r, cache = TRUE}
# Importing teh data directly from Excel
customer.data <- read_excel("C://Users//nelli//Downloads//ltv Dataset.xlsx", sheet = "Sheet1")
```

```{r, cache = TRUE}
# Transform the data to teh desired format
customer.data <- transform(
  customer.data,
  id=as.integer(id),
  status=as.integer(status),
  gender=as.factor(gender),
  date=as.Date(date),
  pages=as.integer(pages),
  onsite=as.integer(onsite),
  entered=as.integer(entered),
  completed=as.integer(completed),
  holiday=as.integer(holiday)
)
```

```{r}
#processing the date
customer.data$date <- as.Date(customer.data$date,'%m/%d/%Y')
customer.data$month <- months(customer.data$date)
customer.data$year <- format(customer.data$date,format = '%Y')
```

```{r}
#calculate the average value
ltv.pages <- aggregate( pages ~ id+ month + year, customer.data, mean)
ltv.onsite <- aggregate( onsite ~ id + month + year, customer.data, mean)

#merger the table
ltv.m1 <- merge(x = ltv.pages, y = ltv.onsite, by = c('id','month','year'), all.x = TRUE)

#find the overall login counts per month
ltv.logins <- aggregate(cbind(count = date) ~ id + month + year, 
          customer.data, 
          FUN = function(x){NROW(x)})
#order this by the id, year and month
ltv.logins <- ltv.logins[
  with(ltv.logins, order(id, year, match(ltv.logins$month, month.name))),]  %>% group_by(id)



```

```{r}
#sort the dataframe and export it
ltv.m1 <- ltv.m1[order(ltv.m1$id),]
ltv.m1
write.csv(ltv.m1,'ltv_modelOne.csv',row.names = FALSE)
```

```{r, cache=TRUE}
#convert the data from numeric to date type (duplicate) 
#customer.data$date <- as.Date(customer.data$date, origin = "1899-12-30")
#Calculate the customer lifespan 
#first group the data by ID to find the max and min date for a given customer
#along with the latest status 
customer.lifespan <- customer.data[, c("id", "date", "status")] %>% group_by(id)
customer.lifespan <-customer.lifespan %>% mutate(maxDate = max(date)) 
#we can asssume that a maxstatus of 0 => that the user just created the account and did not login 
#after, we will consider such a scenario as active
customer.lifespan <-customer.lifespan %>% mutate(status = ifelse(max(status)==0, 1,max(status))) 
customer.lifespan <- customer.lifespan %>% filter(date == min(date)) %>% distinct(id, .keep_all = TRUE) %>% rename(minDate = date)
#Subtract the maxDate and minDate to determine the number of days of subscription
customer.lifespan$subDays <- as.integer(difftime(customer.lifespan$maxDate, customer.lifespan$minDate, units = "days"))
#Determine the observed lifespan factor to be added
lifespanFraction <- 1/(with(1, sum(customer.data$status == 2))/10000)
#calculate the lifespan for the customers
customer.lifespan$lifespan <- customer.lifespan$subDays

#find the max and min Months and years
customer.lifespan$maxMonth <- months(customer.lifespan$maxDate)
customer.lifespan$minYear <- format(customer.lifespan$maxDate,format = '%Y')
customer.lifespan$minMonth <- months(customer.lifespan$minDate)
customer.lifespan$minYear <- format(customer.lifespan$minDate,format = '%Y')


#customer.lifespan$lifespan <- ifelse(customer.lifespan$status == 2, customer.lifespan$subDays, customer.lifespan$subDays + lifespanFraction)
#add this data to the main dataset 
customer.data$lifespan <- customer.lifespan$lifespan[match(customer.data$id,customer.lifespan$id)]


customer.overall.means <- aggregate(customer.data[, c("pages", "onsite", "completed")], list(customer.data$id), mean)
customer.overall.means <- customer.overall.means %>% rename(id = Group.1)
customer.overall.means$lastStatus <- customer.lifespan$status
customer.overall.means$lifespan <- customer.lifespan$lifespan
#determine the logins in the first month, last and the last but one 
customer.firstMonth.logins <- ltv.logins  %>%  filter(row_number()==1) %>% dplyr::select("id", "count")
customer.previousMonth.logins <- ltv.logins  %>%  filter(row_number()==n()-1) %>% dplyr::select("id",  "count")
customer.lastMonth.logins <- ltv.logins  %>%  filter(row_number()==n()) %>% dplyr::select("id", "count")

#determine the login ratios of lastMonth to first and the last month to the last but on
len = length(customer.overall.means$id)

for (i in seq(length(customer.overall.means$id))) {
  curId <- customer.overall.means$id[i]
  lastMonthLogins <- customer.lastMonth.logins[which(customer.lastMonth.logins$id==curId),2]
  firstMonthLogins <- customer.firstMonth.logins[which(customer.firstMonth.logins$id==curId),2]
  prevMonthLogins <- customer.previousMonth.logins[which(customer.previousMonth.logins$id==curId),2]
  if (is.na(prevMonthLogins$count[1])){
    #print(curId)
    prevMonthLogins <- lastMonthLogins
  }
  customer.overall.means$lastToFirstLoginRatio[i] <- as.numeric(lastMonthLogins/firstMonthLogins)
  customer.overall.means$lastToPrevLoginRatio[i] <- as.numeric(lastMonthLogins/prevMonthLogins)
}


```

```{r, cache = TRUE}
# Add 2 columns in the dataframe representing completed/holiday and onsite/entered
ltv_afterProcess <- transform(
  customer.data,
  CompletedVSHoliday=as.integer(entered)/as.integer(holiday),
  OnsiteVSEntered=as.integer(onsite)/as.integer(entered)
)
```

```{r, cache = TRUE}
# calculate the ratio between sum of all entered and sum of all completed
SumEnteredVSCompleted <- sum(ltv_afterProcess$entered)/sum(ltv_afterProcess$completed)
SumEnteredVSCompleted
```

```{r, cache = TRUE}
# create a new dataframe representing teh aggregated summation of each variable per customer
aggregatedCustomerSums <- aggregate(cbind(PagesSum=ltv_afterProcess$pages, OnsiteSum=ltv_afterProcess$onsite, EnteredSum=ltv_afterProcess$entered, CompletedSum=ltv_afterProcess$completed, HolidaySum=ltv_afterProcess$holiday), by=list(Customerid=ltv_afterProcess$id), FUN=sum)
aggregatedCustomerSums<-merge(x = aggregatedCustomerSums, y = customer.data, intersect(names(aggregatedCustomerSums), names(customer.data)), by.x = "Customerid", by.y = "id", all.x=TRUE)[,c(names(aggregatedCustomerSums), "gender")]%>% distinct(Customerid, .keep_all=TRUE)
aggregatedCustomerSums <- transform(
  aggregatedCustomerSums,
  gender = as.factor(gender),
  status = customer.lifespan$status,
  lifespan = customer.lifespan$lifespan,
  decisiveratio = CompletedSum / EnteredSum
)
aggregatedCustomerSums <- rename(aggregatedCustomerSums, id = Customerid, pages = PagesSum, onsite = OnsiteSum, entered = EnteredSum, completed = CompletedSum, holiday = HolidaySum)
```


## 1.	Develop an attrition model, to predict whether a customer will cancel their subscription in the near future. Characterize your model performance.

#First Method: predict whether a customer will cancel their subscription in the near future based on different segments customer

We will try to determine if we can segment the customers into buckets and determine 
if any behaviour related to their average online time, completed orders can be used to 
predict whether a customer will cancel their subscription in the near future

- First pass analysis of the predictors that would be used via Hierarchical Clustering
```{r}
std.customer <- na.omit(customer.overall.means) # listwise deletion of missing
std.customer <- scale(std.customer) # standardize variables
wss <- (nrow(std.customer)-1)*sum(apply(std.customer,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(std.customer,
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
km.res <- kmeans(std.customer, 4, nstart = 25)
str(km.res)
clusplot(std.customer, km.res$cluster, color=TRUE, shade=TRUE,
   labels=2, lines=0)
```

```{r}

set.seed(30)
#Heirarchical Clustering
# we create a 70% data partition from the subset of customers who cancelled
cancelled.customers = createDataPartition(customer.overall.means$id[which(customer.overall.means$lastStatus == 2)],l=F, p = 0.5)
# we create a 30% data partition from the subset of customers who are active
active.customers = createDataPartition(customer.overall.means$id[which(customer.overall.means$lastStatus == 1)],l=F, p = 0.5)

customer.train <- rbind(active.customers, cancelled.customers)

scalled.customer = scale(customer.overall.means[,-1])
cancelled.customers.train = customer.overall.means[customer.train,-1 ]
cancelled.customers.test = customer.overall.means[-customer.train, -1]

scalled.customer.train  = scalled.customer[customer.train, ]
# Compute distance metrics on the standardized customer data
distanceMatrix = dist(scalled.customer.train)

# Perform hierarchical clustering on distance metrics
hierarchical.cluster <- hclust(distanceMatrix, method="ward.D2")
# Build dendrogram object from hierarchical.cluster results
hierarchical.dendogram <- as.dendrogram(hierarchical.cluster)
# Extract the data (for rectangular lines)
hierarchical.dendogram <- dendro_data(hierarchical.dendogram, type = "rectangle")

names(hierarchical.dendogram)
head(hierarchical.dendogram$segments)
head(hierarchical.dendogram$labels)

# Plot the dendogram
p <- ggplot(hierarchical.dendogram$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_text(data = hierarchical.dendogram$labels, aes(x, y, label = label),
            hjust = 1, angle = 90, size = 3)+
  ylim(-3, 15)+ ggtitle("Dendogram for Customer Segmentation")
print(p)
```
```{r}
#The dendogram is too crowded and so we first make 5 segments and view the 
#data
members = cutree(hierarchical.cluster, k = 5)
aggregate(cancelled.customers.train, by = list(members), mean)
```
 - We can try to understand the profile of each segment.
 - Customer falling under segment/group 5 are likely to leave the subscription (in a month or two). The number of pages visited on average is the least and their life span is large. These are customers who most likely got bored or found other resources. The latter ratio of last month to prev seems to have increased by almost 100% when comapred to the last to first month ratio. 
 - Similarly, we see how customers belonging to the segment/group 4 have an average status towards 2 i.e cancelled. Their last month to first month login ratio as well as the prev to last month ratio is the highest. We can say that customers that come under this segment are likely to unscribe from the e-card greeting subscription. 

- Let us try increasing the segments to 4 to see if more distinct status based segments can be obtained 
```{r}
# increasing the segments to 3
members = cutree(hierarchical.cluster, k = 4)
segmented.customers <- as.data.frame(aggregate(cancelled.customers.train, by = list(members), mean))
segmented.customers <- segmented.customers %>% rename(Type = Group.1)
segmented.customers$Type <- c("Sleeping", "Cancelled", "Napping","Active" )
segmented.customers
```
- With 4 segments we can see clearer segments forming with distinct behaviour.
 - In general we notice that as the lifespan increases customers tend to stop/cancel the subscription. This is alarming to the company as it is clear that more effort must be maintained in retaining current subscribers. A clear indication that customers would leave based on increasing lifespan is the difference between the two ratios. Larger the lastToPrevLogin ratio compared to the lastToFirst the more likely that the customer would stop the subscription. 
 - However, we notice that customers are more likely to stay on, even if their lifespan on average is large, if their browsing ratios are maintained within a 50% to 80% range.

# Predicting via SVM
```{r, cache=TRUE}
cancelled.svm <-svm(as.factor(cancelled.customers.train$lastStatus) ~ . , data = cancelled.customers.train)
#cancelled.svm

cancelled.svm.pred <- predict(cancelled.svm, cancelled.customers.test)
conf.mat.cancelled.svm<-table(cancelled.svm.pred, cancelled.customers.test$lastStatus)
conf.mat.cancelled.svm

cancelled.svm.accuracy<- sum(diag(conf.mat.cancelled.svm)) / sum(conf.mat.cancelled.svm)
cancelled.svm.accuracy

#Parameter tuning 
#cancelled.svm.tuned <-tune.svm(cancelled.customers.train[,-4], as.factor(cancelled.customers.train$lastStatus), gamma=10^(-6:-3), cost=2^(2:9))
#print(cancelled.svm.tuned)

#plot(cancelled.svm.tuned)

#cancelled.svm.best <- cancelled.svm.tuned$best.model
#cancelled.svm.best.pred <- predict(cancelled.svm.best, cancelled.customers.test) 
 
#best.model.error <- as.factor(cancelled.customers.train$lastStatus) - cancelled.svm.best.pred 
#RMSE 
#sqrt(mean(best.model.error^2)) 
```
```


# Second Method: predict the specific lifespan of each customer

```{r}
# splitting prediction training and testing dataset
customer.canceled.train <- aggregatedCustomerSums[aggregatedCustomerSums$status == "2",]
customer.continuing.test <- aggregatedCustomerSums[aggregatedCustomerSums$status != "2",]

# process prediction variables
drops <- c("id","status","decisiveratio")
customer.canceled.train <- customer.canceled.train[ , !(names(customer.canceled.train) %in% drops)]
customer.continuing.test <- customer.continuing.test[ , !(names(customer.continuing.test) %in% drops)]
```

```{r}
# linear lasso model

customer.met2.lasso.x <- model.matrix(lifespan~., customer.canceled.train)[,-1]
customer.met2.lasso.test <- model.matrix(lifespan~., customer.continuing.test)[,-1]
# 37 features
# dim(ltv.x) 
customer.met2.lasso.y <- customer.canceled.train$lifespan

customer.met2.lasso.reg <- glmnet(customer.met2.lasso.x, customer.met2.lasso.y, alpha = 1)
summary(customer.met2.lasso.reg)

lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
customer.met2.lasso.cvreg <- cv.glmnet(customer.met2.lasso.x, customer.met2.lasso.y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- customer.met2.lasso.cvreg$lambda.min 
lambda_best

predict(customer.met2.lasso.cvreg, newx=customer.met2.lasso.test, s = lambda_best, type="response")
```

```{r}
# linear model

```

## 2.	Develop a model for estimating the ltv of a customer. Characterize your model performance.

```{r}
```

## 3.	Develop a customer segmentation scheme. Include in this scheme the identification of sleeping customers, those that are no longer active but have not canceled their account.

# Segmentation 1: Segmentation 1: Regular(users who have completed regularly irrespective of holidays), Sleeping, Holiday User(holiday based customers),  Inactive (canceled)

- Variables: Completed, pages, onsite, entered
- Classification label: Regular/Sleeping/Inactive
  - 3: if the customer has browsed the website >60 days of `12/31/2014` then classify the customer as sleeping.
  - 2: if all the variables of related to that customer is 0 then we classfy the customer as a inactive.
  - 1: if the customer has recently browsed the website has been active within 60 days of `12/31/2014` then classify the customer as active.

- data preprocssing:
```{r}
aggregatedCustomerSums$holiday.ratio <- aggregatedCustomerSums$holiday/aggregatedCustomerSums$completed
summary(aggregatedCustomerSums$holiday.ratio)
```
```{r}
aggregatedCustomerSums$holiday.user <- ifelse(aggregatedCustomerSums$holiday.ratio>0.4573,1,0)
```

active = 1
Sleeping = 2
Inactive (canceled) = 3
```{r}
width = length(customer.data)
len = length(customer.data$id)
user.type = rep(0, times = len)
previd = customer.data[1][1]
startid = previd
for(i in seq(len)){
  startid = customer.data[i,1]
  if(customer.data[i,2] == 2){
    user.type[i] = 3
  }else if(startid!=previd){
    diff = (as.Date('12/31/2014','%m/%d/%Y') - customer.data[i-1,4])
    if(diff>60){
      user.type[i-1] = 2
      user.type[i] = 1
    }else{
      user.type[i] = 1
    }
  }else{
    user.type[i] = 1
  }
  previd = startid
}
customer.data$userType <- user.type
```


```{r}
#select the data we need, we do not want to overfit the model, and hence will only include data only the 
# maxDate related data for the each user.
customer.data.segOne <- customer.data %>% group_by(id) %>% filter(date == max(date)) 
customer.data.segOne <- subset(customer.data.segOne,select = c('pages','onsite', 'userType'))#, 'entered', 'completed'))
#split into training and test dataset
customer.data.segOne <- transform(
  customer.data.segOne,
  pages=as.integer(pages),
  onsite=as.integer(onsite),
  userType = as.factor(userType)
)
dt = sort(sample(nrow(customer.data.segOne), nrow(customer.data.segOne)*.7))
train.segOne<-customer.data.segOne[dt,]
test.segOne<-customer.data.segOne[-dt,]

```

#random forest
```{r}
set.seed(42)
customer.segOne.rf <- ranger(
  as.factor(userType) ~ ., 
  data = train.segOne, 
  importance = "impurity",
  num.trees = 2000
)
customer.segOne.pred <- predict(customer.segOne.rf, test.segOne)
customer.segOne.rfResult<- table(test.segOne$userType, predictions(customer.segOne.pred))
customer.segOne.rfResult
accuracy<- sum(diag(customer.segOne.rfResult)) / sum(customer.segOne.rfResult)
accuracy
```

# Logistic Regression

```{r}
#create the logistic regression model with output as userType and inputs as pages, onsite, entered and completed.
customer.segOne.lr <- multinom(train.segOne$userType ~ . , data = train.segOne)
customer.segOne.lr
#test the model classification on the test data
customer.segOne.lr.pred <- predict(customer.segOne.lr,newdata=test.segOne)
#create a confusion matrix
customer.segOne.lr.conf <-table(customer.segOne.lr.pred, test.segOne$userType)
prop.table(customer.segOne.lr.conf)
#determine the accuracy of the model
customer.segOne.lr.accuracy<- sum(diag(customer.segOne.lr.conf)) / sum(customer.segOne.lr.conf)
customer.segOne.lr.accuracy
```

# Naive Bayes

```{r}
#Construct a naive bayes classification model on the train data set
customer.segOne.nb <-naiveBayes(train.segOne$userType ~ ., data = train.segOne)
customer.segOne.nb
#test the model classification on the test data
customer.segOne.nb.pred <- predict(customer.segOne.nb, newdata=test.segOne)
#create a confusion matrix
customer.segOne.nb.conf <-table(customer.segOne.nb.pred, test.segOne$userType)
prop.table(customer.segOne.nb.conf)
#determine the accuracy of the model
customer.segOne.nb.accuracy<- sum(diag(customer.segOne.nb.conf)) / sum(customer.segOne.nb.conf)
customer.segOne.nb.accuracy
```

=======

#test the model classification on the test data
customer.segOne.nb.pred <- predict(customer.segOne.nb, newdata=test.segOne)

#create a confusion matrix
customer.segOne.nb.conf <-table(customer.segOne.nb.pred, test.segOne$userType)
prop.table(customer.segOne.nb.conf)

#determine the accuracy of the model
customer.segOne.nb.accuracy<- sum(diag(customer.segOne.nb.conf)) / sum(customer.segOne.nb.conf)
customer.segOne.nb.accuracy

>>>>>>> 0dd6f9099c5315c95a1fd5e0a66e27ff9e8f8614
- Segmentation 2: Decisive customers, Tentative customers, Hesitant customers
- Variables: Gender, onsite, pages, entered, completed
- Classification label: completed / entered 
  - 2: if the `decisiveratio` of that customer is greater or equal to `r summary(aggregatedCustomerSums$decisiveratio)[5]` then we classfy the customer as decisive
  - 1: if the `decisiveratio` of that customer fall in the range of `r summary(aggregatedCustomerSums$decisiveratio)[2]` and `r summary(aggregatedCustomerSums$decisiveratio)[5]`, then we classfy the customer as tentative
  - 0: if the `decisiveratio` of that customer is smaller than `r summary(aggregatedCustomerSums$decisiveratio)[2]` , then we classfy the customer as hesitant

```{r}
summary(aggregatedCustomerSums$decisiveratio)
```


```{r}
# create data labels 
decisivelabel <- with(aggregatedCustomerSums, ifelse(decisiveratio >= summary(aggregatedCustomerSums$decisiveratio)[5], 2, ifelse(decisiveratio >= summary(aggregatedCustomerSums$decisiveratio)[2],1,0)))
# merge the created label with selected data
decisive.data <- data.frame(aggregatedCustomerSums[c("pages", "onsite","holiday","gender","lifespan")], decisivelabel = as.factor(decisivelabel))
set.seed(42)
# separate data into train, test data
#randomly get 2/3 data of each label into train data, 1/3 data of each label into test data
hesitant<-subset(decisive.data, decisivelabel == 0)
tentative<-subset(decisive.data, decisivelabel == 1)
decisive<-subset(decisive.data, decisivelabel == 2)
train.h<-hesitant[sample(nrow(hesitant),), ][c(1:1658), ]
test.h<-hesitant[sample(nrow(hesitant),), ][c(1659:2487), ]
train.t<-tentative[sample(nrow(tentative),), ][c(1:3155),]
test.t<-tentative[sample(nrow(tentative),), ][c(3156:4732),]
train.d<-decisive[sample(nrow(decisive),), ][c(1:1854),]
test.d<-decisive[sample(nrow(decisive),), ][c(1854:2781),]
decisive.train<-rbind(train.h, train.t, train.d)
decisive.test<-rbind(test.h, test.t, test.d)
```

```{r, cache = TRUE}
# Random Forest
# build the random forest model for classification
customer.rf <- randomForest(decisivelabel~., data=decisive.train, ntree=500, proximity=T)
customer.rf
customer.rf.predict <- predict(customer.rf, decisive.test)
customer.rf.table <- table(customer.rf.predict, decisive.test$decisivelabel)
customer.rf.table
# calculate the accuracy of our random forest model
customer.rf.misclassificationrate = mean(customer.rf.predict != decisive.test$decisivelabel)
customer.rf.accuracy <- 1- customer.rf.misclassificationrate
customer.rf.accuracy
```


```{r}
# KNN
train_control <- trainControl(method = "cv", number = 10)
customer.knn <- train(decisivelabel~., data = decisive.train, trControl = train_control, method = "knn")
customer.knn
customer.knn.predict <- predict(customer.knn, decisive.test)
customer.knn.table <- table(customer.knn.predict, decisive.test$decisivelabel)
customer.knn.table
# calculate the accuracy of our random forest model
customer.knn.misclassificationrate = mean(customer.knn.predict != decisive.test$decisivelabel)
customer.knn.accuracy <- 1- customer.knn.misclassificationrate
customer.knn.accuracy
```

```{r}
# Mulrinomial Logistic Regression
decisve.ml <- multinom(decisivelabel ~ . , data = decisive.train)
decisve.ml
logit.pred <- decisve.ml %>% predict(decisive.test)
conf.mat.logit<-table(logit.pred, decisive.test$decisivelabel)
conf.mat.logit
logit.accuracy<- sum(diag(conf.mat.logit)) / sum(conf.mat.logit)
logit.accuracy
```

```{r}
# Naive Bayes
decisive.nb <-naiveBayes(decisivelabel ~ . , data = decisive.train, laplace=1)
decisive.nb.pred <- predict(decisive.nb, decisive.test)
conf.mat.nb<-table(decisive.nb.pred, decisive.test$decisivelabel)
conf.mat.nb
nb.accuracy<- sum(diag(conf.mat.nb)) / sum(conf.mat.nb)
nb.accuracy
```

```{r}
#SVM
decisive.svm <-svm(decisivelabel ~ . , data = decisive.train, type = 'C-classification', kernel = 'radial')
decisive.svm.pred <- predict(decisive.svm, decisive.test)
conf.mat.svm<-table(decisive.svm.pred, decisive.test$decisivelabel)
conf.mat.svm
svm.accuracy<- sum(diag(conf.mat.svm)) / sum(conf.mat.svm)
svm.accuracy
```

## Random forest is the best
